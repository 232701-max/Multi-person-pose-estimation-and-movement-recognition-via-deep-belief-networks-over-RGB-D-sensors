# Multi-person-pose-estimation-and-movement-recognition-via-deep-belief-networks-over-RGB-D-sensors
This project presents an advanced Human Movement Recognition (HMR) framework using Deep Belief Networks (DBNs) with spatio-temporal feature extraction from RGB-D video sensors.


Overview
This project presents an advanced Human Movement Recognition (HMR) framework using Deep Belief Networks (DBNs) with spatio-temporal feature extraction from RGB-D video sensors. The system integrates full-body and skeletal motion analysis to classify human activities with high accuracy. The model is evaluated on ISR UOL 3D, NTU RGB+D 120, and PKU-MMD datasets, demonstrating superior recognition performance compared to state-of-the-art methods.

Code Explanation
The project consists of multiple key components, each contributing to the overall recognition framework:
1.	Preprocessing
o	Keyframe Selection: Identifies significant frames using optical flow-based motion analysis.
o	Noise Reduction: Applies Non-Local Means (NLM) filtering to RGB images and Gaussian smoothing to depth images.
o	Normalization: Standardizes pixel intensities for robustness.
o	ROI Extraction: Utilizes Gaussian Mixture Model (MOG2) for dynamic foreground segmentation.
2.	Segmentation
o	RGB Data: U-Net is used for human silhouette extraction.
o	Depth Data: Affine transformations align depth silhouettes and refine spatial consistency.
3.	Skeleton and Key Point Generation
o	Skeletal Data: Extracted using MediaPipe for RGB and Geodesic Distance for Depth.
4.	Feature Extraction
o	RGB Features: Kinetic Energy, Ridge Features, and Kinematic Postures.
o	Depth Features: Random Occupancy Patterns (ROP), 2.5D Point Clouds, and Fast Point Feature Histograms (FPFH).
5.	Feature Optimization
o	Particle Swarm Optimization (PSO) selects the most discriminative features for classification.
6.	Classification with DBN
A Deep Belief Network (DBN) is trained on optimized features to classify multi-person activities efficiently.

Data Availability Statement (for manuscript)


The study utilized three publicly available datasets:
•	ISR UOL 3D Social Activity Dataset (Coppola et al., 2016). Available at: https://lcas.lincoln.ac.uk/wp/research/data-sets-software/isr-uol-3d-social-activity-dataset/
•	NTU RGB+D 120 Dataset (Liu et al., 2019). Available at: https://rose1.ntu.edu.sg/dataset/actionRecognition/
•	PKU-MMD Dataset (Liu et al., 2017). Available at: https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html
All datasets are third-party resources and not generated by the authors. DOIs and references are provided below.

Dataset Citations
1.	Coppola, C., Faria, D. R., Nunes, U., & Bellotto, N. (2016). Social activity recognition based on probabilistic merging of skeleton features with proximity priors from RGB-D data. 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 5055–5061. https://doi.org/10.1109/IROS.2016.7759742
2.	Liu, J., Shahroudy, A., Perez, M., Wang, G., Duan, L., & Kot, A. C. (2019). NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(10), 2684–2701. https://doi.org/10.1109/TPAMI.2019.2916873
3.	Liu, H., Tu, H., Liu, M., Yuan, J., & Chen, C. (2017). PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding. IEEE International Conference on Multimedia and Expo (ICME), pp. 801–9573. https://doi.org/10.1109/ICME.2017.8019573
Confidential Note to Staff (Raw Data Repository requirement)
The data analyzed in this study are third-party public datasets and were not generated by the authors. Accordingly, DOIs and access details are provided as follows:
•	ISR UOL 3D Social Activity Dataset (Coppola et al., 2016): https://lcas.lincoln.ac.uk/wp/research/data-sets-software/isr-uol-3d-social-activity-dataset/
•	NTU RGB+D 120 (Liu et al., 2019): https://rose1.ntu.edu.sg/dataset/actionRecognition/
•	PKU-MMD (Liu et al., 2017): https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html

